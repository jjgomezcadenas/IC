{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs. background classification with NEW full MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.patches         import Ellipse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy  as np\n",
    "import random as rd\n",
    "import tables as tb\n",
    "import h5py\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "from __future__  import print_function\n",
    "from scipy.stats import threshold\n",
    "\n",
    "from keras.models               import Model, load_model\n",
    "from keras.layers               import Input, Dense, MaxPooling3D, AveragePooling3D, Convolution3D, Activation, Dropout, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers           import SGD, Adam, Nadam         \n",
    "#from keras.callbacks            import ReduceLROnPlateau !!!!!\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core          import Flatten\n",
    "from keras                      import callbacks\n",
    "from keras.regularizers         import l2, activity_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jrenner/data/classification/NEW_training_MC_si_20.h5 (File) ''\n",
      "Last modif.: 'Wed Dec 28 06:44:26 2016'\n",
      "Object Tree: \n",
      "/ (RootGroup) ''\n",
      "/energies (EArray(20000, 30), blosc(9)) ''\n",
      "/maps (EArray(20000, 20, 20, 30), blosc(9)) ''\n",
      "\n",
      "/home/jrenner/data/classification/NEW_training_MC_bg_20.h5 (File) ''\n",
      "Last modif.: 'Wed Dec 28 08:16:46 2016'\n",
      "Object Tree: \n",
      "/ (RootGroup) ''\n",
      "/energies (EArray(20000, 30), blosc(9)) ''\n",
      "/maps (EArray(20000, 20, 20, 30), blosc(9)) ''\n",
      "\n",
      "Concatenating datasets...\n",
      "Reshaping...\n",
      "Prepared 34000 training events,  4000 validation events, 2000 test events\n"
     ]
    }
   ],
   "source": [
    "Ntrain = 17000     # number of training events per sample\n",
    "Ntest = 1000\n",
    "Ntot = 20000\n",
    "xdim = 20\n",
    "ydim = 20\n",
    "zdim = 30\n",
    "\n",
    "# Signal events.\n",
    "s_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_si_20.h5', 'r')\n",
    "#s_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_si_nst_nonorm.h5', 'r')\n",
    "print(s_dat)\n",
    "s_array = np.array(s_dat.root.maps)\n",
    "x_t = s_array[:Ntrain]\n",
    "x_v = s_array[Ntrain:Ntot-Ntest]\n",
    "x_e = s_array[Ntot-Ntest:Ntot]\n",
    "y_t = np.ones([Ntrain, 1])\n",
    "y_v = np.ones([Ntot-Ntrain-Ntest, 1])\n",
    "y_e = np.ones([Ntest, 1])\n",
    "\n",
    "s_earray = np.array(s_dat.root.energies)\n",
    "\n",
    "# Background events.\n",
    "b_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_bg_20.h5', 'r')\n",
    "#b_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_bg_nst_nonorm.h5', 'r')\n",
    "print(b_dat)\n",
    "b_array = np.array(b_dat.root.maps)\n",
    "print(\"Concatenating datasets...\")\n",
    "x_t = np.concatenate([x_t, b_array[:Ntrain]])\n",
    "x_v = np.concatenate([x_v, b_array[Ntrain:Ntot-Ntest]])\n",
    "x_e = np.concatenate([x_e, b_array[Ntot-Ntest:Ntot]])\n",
    "y_bt = np.zeros([Ntrain, 1])\n",
    "y_t = np.concatenate([y_t, y_bt])\n",
    "y_bv = np.zeros([Ntot-Ntrain-Ntest, 1])\n",
    "y_v = np.concatenate([y_v, y_bv])\n",
    "y_be = np.zeros([Ntest, 1])\n",
    "y_e = np.concatenate([y_e, y_be])\n",
    "\n",
    "b_earray = np.array(b_dat.root.energies)\n",
    "\n",
    "# Normalize\n",
    "#mval = max(np.max(s_array),np.max(b_array))\n",
    "#muval = np.mean(s_array)\n",
    "#sval = np.std(s_array)\n",
    "#print(\"Normalizing with max value of\", mval, \"(mean of\", muval, \"; sigma of \", sval, \")\")\n",
    "#x_t /= sval\n",
    "#x_v /= sval\n",
    "\n",
    "# Include the final dimension (single-channel).\n",
    "print(\"Reshaping...\")\n",
    "#x_t = np.expand_dims(x_t, axis=1)\n",
    "#x_v = np.expand_dims(x_v, axis=1)\n",
    "x_t = np.reshape(x_t, (len(x_t), xdim, ydim, zdim, 1))\n",
    "x_v = np.reshape(x_v, (len(x_v), xdim, ydim, zdim, 1))\n",
    "x_e = np.reshape(x_e, (len(x_e), xdim, ydim, zdim, 1))\n",
    "print(\"Prepared\", len(x_t), \"training events, \", len(x_v), \"validation events,\", len(x_e), \"test events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove noise (simple cut).\n",
    "nthr = 5.0e-5\n",
    "inoise = x_t < nthr\n",
    "x_t[inoise] = 0\n",
    "\n",
    "inoise = x_v < nthr\n",
    "x_v[inoise] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove noise (complex cut).\n",
    "nsum_thr = 0.01\n",
    "\n",
    "# Remove noise from training events.\n",
    "print(\"Removing noise from training events...\")\n",
    "for enum in range(len(x_t)):\n",
    "    for nx in range(xdim):\n",
    "        for ny in range(ydim):\n",
    "            sval = np.sum(x_t[enum,nx,ny,:])\n",
    "            if(sval < nsum_thr):\n",
    "                x_t[enum,nx,ny,:] = 0\n",
    "                \n",
    "# Remove noise from validation events.\n",
    "print(\"Removing noise from validation events...\")\n",
    "for enum in range(len(x_v)):\n",
    "    for nx in range(xdim):\n",
    "        for ny in range(ydim):\n",
    "            sval = np.sum(x_v[enum,nx,ny,:])\n",
    "            if(sval < nsum_thr):\n",
    "                x_v[enum,nx,ny,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 20, 20, 30, 1) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_5 (Convolution3D)  (None, 5, 5, 10, 256) 32256       input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling3d_3 (MaxPooling3D)    (None, 3, 3, 5, 256)  0           convolution3d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_3 (BatchNorma (None, 3, 3, 5, 256)  512         maxpooling3d_3[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_6 (Convolution3D)  (None, 3, 3, 5, 64)   16448       batchnormalization_3[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_7 (Convolution3D)  (None, 2, 2, 2, 128)  98432       convolution3d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_4 (BatchNorma (None, 2, 2, 2, 128)  256         convolution3d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_8 (Convolution3D)  (None, 2, 2, 2, 128)  16512       batchnormalization_4[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling3d_4 (MaxPooling3D)    (None, 1, 1, 1, 128)  0           convolution3d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 128)           0           maxpooling3d_4[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 128)           16512       flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 128)           0           dense_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 1)             129         dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 181057\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lmodel = True\n",
    "model_type = 0\n",
    "\n",
    "with K.tf.device('/gpu:1'):\n",
    "    K.set_session(K.tf.Session(config=K.tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)))\n",
    "    \n",
    "    if(lmodel):\n",
    "        incep = load_model('models/conv3d_classifier_20.h5')\n",
    "    elif(model_type == 0):\n",
    "\n",
    "        inputs = Input(shape=(xdim, ydim, zdim, 1))\n",
    "        cinputs = Convolution3D(256, 5, 5, 5, border_mode='same', subsample=(4, 4, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution3D(128, 2, 2, 3, border_mode='same', subsample=(2, 2, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(128, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=128, activation='relu', init='lecun_uniform', W_regularizer=l2(0.001))(f1)\n",
    "        f1 = Dropout(.7)(f1)\n",
    "\n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='normal', W_regularizer=l2(0.001))(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "\n",
    "        incep.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Nadam(lr=0.000005, beta_1=0.9, beta_2=0.999,\n",
    "                                      epsilon=1e-08, schedule_decay=0.1), metrics=['accuracy'])\n",
    "    elif(model_type == 1):\n",
    "        \n",
    "        # Input layer\n",
    "        inputs = Input(shape=(xdim, ydim, zdim, 1))\n",
    "        \n",
    "        # Initial convolution and normalization.\n",
    "        cinputs = Convolution3D(64, 4, 4, 4, border_mode='same', subsample=(2, 2, 2), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        \n",
    "        # 2 inception layers\n",
    "        i1 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i2_1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i2_2 = Convolution3D(128, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i2_1)\n",
    "        i3_1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i3_2 = Convolution3D(64, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i3_1)\n",
    "        i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        i4_2 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i4_1)\n",
    "        cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        i1 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i2_1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i2_2 = Convolution3D(128, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i2_1)\n",
    "        i3_1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i3_2 = Convolution3D(64, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i3_1)\n",
    "        i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        i4_2 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i4_1)\n",
    "        cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        # Max pooling\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        \n",
    "        # 2 more inception layers\n",
    "        #i1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_1 = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_2 = Convolution3D(64, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i2_1)\n",
    "        #i3_1 = Convolution3D(8, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i3_2 = Convolution3D(32, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i3_1)\n",
    "        #i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #i4_2 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i4_1)\n",
    "        #cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        #i1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_1 = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_2 = Convolution3D(64, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i2_1)\n",
    "        #i3_1 = Convolution3D(8, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i3_2 = Convolution3D(32, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i3_1)\n",
    "        #i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #i4_2 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i4_1)\n",
    "        #cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        # Max pooling\n",
    "        #cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "\n",
    "        # 2 more inception layers\n",
    "        #i1 = Convolution3D(48, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_1 = Convolution3D(24, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_2 = Convolution3D(96, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i2_1)\n",
    "        #i3_1 = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i3_2 = Convolution3D(64, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i3_1)\n",
    "        #i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #i4_2 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i4_1)\n",
    "        #cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        #i1 = Convolution3D(48, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_1 = Convolution3D(24, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_2 = Convolution3D(96, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i2_1)\n",
    "        #i3_1 = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i3_2 = Convolution3D(64, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i3_1)\n",
    "        #i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #i4_2 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i4_1)\n",
    "        #cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        # Average pooling\n",
    "        #cinputs = AveragePooling3D(pool_size=(4, 4, 5), strides=(4, 4, 5), border_mode='valid', dim_ordering='default')(cinputs)\n",
    "        \n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=16, activation='relu', init='lecun_uniform', W_regularizer=l2(0.0001))(f1)\n",
    "        f1 = Dropout(.6)(f1)\n",
    "        \n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='normal', W_regularizer=l2(0.0005))(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "\n",
    "        incep.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Nadam(lr=0.0005, beta_1=0.9, beta_2=0.999,\n",
    "                                      epsilon=1e-08, schedule_decay=0.01), metrics=['accuracy'])\n",
    "        \n",
    "            #callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min'), \n",
    "    lcallbacks = [callbacks.ModelCheckpoint('models/conv3d_classifier_20.h5', monitor='val_loss', save_best_only=True, mode='min')]            \n",
    "    incep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34000 samples, validate on 4000 samples\n",
      "Epoch 1/40\n",
      "34000/34000 [==============================] - 8s - loss: 0.4122 - acc: 0.8680 - val_loss: 0.5101 - val_acc: 0.7792\n",
      "Epoch 2/40\n",
      "32300/34000 [===========================>..] - ETA: 0s - loss: 0.4089 - acc: 0.8684"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-4136a0f66b14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mincep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jrenner/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1109\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1110\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1111\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jrenner/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jrenner/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1094\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m         \u001b[0mupdated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jrenner/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 717\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    718\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jrenner/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 915\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    916\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jrenner/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 965\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/jrenner/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    970\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jrenner/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    953\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = incep.fit(x_t, y_t, shuffle=True, nb_epoch=40, batch_size=100, verbose=1, validation_data=(x_v, y_v), callbacks=lcallbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s     \n",
      "[0.47794937539100646, 0.79100000000000004]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = incep.evaluate(x_e, y_e);\n",
    "y_pred = incep.predict(x_e, batch_size=100, verbose=0)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 0 of 1000 (0.0%) correct background events; 1000 of 1000 (100.0%) correct signal events\n",
      "-- 0 of 1000 (0.0%) correct background events; 1000 of 1000 (100.0%) correct signal events\n",
      "-- 3 of 1000 (0.3%) correct background events; 1000 of 1000 (100.0%) correct signal events\n",
      "-- 11 of 1000 (1.0999999999999999%) correct background events; 1000 of 1000 (100.0%) correct signal events\n",
      "-- 36 of 1000 (3.5999999999999996%) correct background events; 998 of 1000 (99.8%) correct signal events\n",
      "-- 56 of 1000 (5.6000000000000005%) correct background events; 996 of 1000 (99.6%) correct signal events\n",
      "-- 84 of 1000 (8.4%) correct background events; 992 of 1000 (99.2%) correct signal events\n",
      "-- 111 of 1000 (11.1%) correct background events; 989 of 1000 (98.9%) correct signal events\n",
      "-- 140 of 1000 (14.000000000000002%) correct background events; 986 of 1000 (98.6%) correct signal events\n",
      "-- 186 of 1000 (18.6%) correct background events; 984 of 1000 (98.4%) correct signal events\n",
      "-- 218 of 1000 (21.8%) correct background events; 979 of 1000 (97.89999999999999%) correct signal events\n",
      "-- 252 of 1000 (25.2%) correct background events; 975 of 1000 (97.5%) correct signal events\n",
      "-- 270 of 1000 (27.0%) correct background events; 970 of 1000 (97.0%) correct signal events\n",
      "-- 294 of 1000 (29.4%) correct background events; 965 of 1000 (96.5%) correct signal events\n",
      "-- 328 of 1000 (32.800000000000004%) correct background events; 962 of 1000 (96.2%) correct signal events\n",
      "-- 360 of 1000 (36.0%) correct background events; 960 of 1000 (96.0%) correct signal events\n",
      "-- 393 of 1000 (39.300000000000004%) correct background events; 954 of 1000 (95.39999999999999%) correct signal events\n",
      "-- 418 of 1000 (41.8%) correct background events; 947 of 1000 (94.69999999999999%) correct signal events\n",
      "-- 447 of 1000 (44.7%) correct background events; 942 of 1000 (94.19999999999999%) correct signal events\n",
      "-- 474 of 1000 (47.4%) correct background events; 937 of 1000 (93.7%) correct signal events\n",
      "-- 488 of 1000 (48.8%) correct background events; 929 of 1000 (92.9%) correct signal events\n",
      "-- 516 of 1000 (51.6%) correct background events; 920 of 1000 (92.0%) correct signal events\n",
      "-- 528 of 1000 (52.800000000000004%) correct background events; 915 of 1000 (91.5%) correct signal events\n",
      "-- 546 of 1000 (54.6%) correct background events; 906 of 1000 (90.60000000000001%) correct signal events\n",
      "-- 567 of 1000 (56.699999999999996%) correct background events; 899 of 1000 (89.9%) correct signal events\n",
      "-- 579 of 1000 (57.9%) correct background events; 898 of 1000 (89.8%) correct signal events\n",
      "-- 600 of 1000 (60.0%) correct background events; 896 of 1000 (89.60000000000001%) correct signal events\n",
      "-- 609 of 1000 (60.9%) correct background events; 892 of 1000 (89.2%) correct signal events\n",
      "-- 621 of 1000 (62.1%) correct background events; 889 of 1000 (88.9%) correct signal events\n",
      "-- 630 of 1000 (63.0%) correct background events; 883 of 1000 (88.3%) correct signal events\n",
      "-- 637 of 1000 (63.7%) correct background events; 881 of 1000 (88.1%) correct signal events\n",
      "-- 645 of 1000 (64.5%) correct background events; 878 of 1000 (87.8%) correct signal events\n",
      "-- 651 of 1000 (65.10000000000001%) correct background events; 873 of 1000 (87.3%) correct signal events\n",
      "-- 660 of 1000 (66.0%) correct background events; 873 of 1000 (87.3%) correct signal events\n",
      "-- 672 of 1000 (67.2%) correct background events; 870 of 1000 (87.0%) correct signal events\n",
      "-- 681 of 1000 (68.10000000000001%) correct background events; 866 of 1000 (86.6%) correct signal events\n",
      "-- 691 of 1000 (69.1%) correct background events; 863 of 1000 (86.3%) correct signal events\n",
      "-- 693 of 1000 (69.3%) correct background events; 862 of 1000 (86.2%) correct signal events\n",
      "-- 697 of 1000 (69.69999999999999%) correct background events; 858 of 1000 (85.8%) correct signal events\n",
      "-- 701 of 1000 (70.1%) correct background events; 855 of 1000 (85.5%) correct signal events\n",
      "-- 704 of 1000 (70.39999999999999%) correct background events; 852 of 1000 (85.2%) correct signal events\n",
      "-- 712 of 1000 (71.2%) correct background events; 848 of 1000 (84.8%) correct signal events\n",
      "-- 720 of 1000 (72.0%) correct background events; 846 of 1000 (84.6%) correct signal events\n",
      "-- 729 of 1000 (72.89999999999999%) correct background events; 845 of 1000 (84.5%) correct signal events\n",
      "-- 737 of 1000 (73.7%) correct background events; 844 of 1000 (84.39999999999999%) correct signal events\n",
      "-- 741 of 1000 (74.1%) correct background events; 840 of 1000 (84.0%) correct signal events\n",
      "-- 746 of 1000 (74.6%) correct background events; 835 of 1000 (83.5%) correct signal events\n",
      "-- 755 of 1000 (75.5%) correct background events; 830 of 1000 (83.0%) correct signal events\n",
      "-- 761 of 1000 (76.1%) correct background events; 826 of 1000 (82.6%) correct signal events\n",
      "-- 766 of 1000 (76.6%) correct background events; 819 of 1000 (81.89999999999999%) correct signal events\n",
      "-- 769 of 1000 (76.9%) correct background events; 813 of 1000 (81.3%) correct signal events\n",
      "-- 777 of 1000 (77.7%) correct background events; 806 of 1000 (80.60000000000001%) correct signal events\n",
      "-- 780 of 1000 (78.0%) correct background events; 803 of 1000 (80.30000000000001%) correct signal events\n",
      "-- 785 of 1000 (78.5%) correct background events; 796 of 1000 (79.60000000000001%) correct signal events\n",
      "-- 788 of 1000 (78.8%) correct background events; 789 of 1000 (78.9%) correct signal events\n",
      "-- 794 of 1000 (79.4%) correct background events; 786 of 1000 (78.60000000000001%) correct signal events\n",
      "-- 796 of 1000 (79.60000000000001%) correct background events; 783 of 1000 (78.3%) correct signal events\n",
      "-- 800 of 1000 (80.0%) correct background events; 774 of 1000 (77.4%) correct signal events\n",
      "-- 808 of 1000 (80.80000000000001%) correct background events; 763 of 1000 (76.3%) correct signal events\n",
      "-- 813 of 1000 (81.3%) correct background events; 758 of 1000 (75.8%) correct signal events\n",
      "-- 819 of 1000 (81.89999999999999%) correct background events; 753 of 1000 (75.3%) correct signal events\n",
      "-- 826 of 1000 (82.6%) correct background events; 748 of 1000 (74.8%) correct signal events\n",
      "-- 827 of 1000 (82.69999999999999%) correct background events; 739 of 1000 (73.9%) correct signal events\n",
      "-- 835 of 1000 (83.5%) correct background events; 730 of 1000 (73.0%) correct signal events\n",
      "-- 837 of 1000 (83.7%) correct background events; 727 of 1000 (72.7%) correct signal events\n",
      "-- 842 of 1000 (84.2%) correct background events; 720 of 1000 (72.0%) correct signal events\n",
      "-- 847 of 1000 (84.7%) correct background events; 708 of 1000 (70.8%) correct signal events\n",
      "-- 851 of 1000 (85.1%) correct background events; 700 of 1000 (70.0%) correct signal events\n",
      "-- 858 of 1000 (85.8%) correct background events; 691 of 1000 (69.1%) correct signal events\n",
      "-- 863 of 1000 (86.3%) correct background events; 682 of 1000 (68.2%) correct signal events\n",
      "-- 866 of 1000 (86.6%) correct background events; 675 of 1000 (67.5%) correct signal events\n",
      "-- 871 of 1000 (87.1%) correct background events; 664 of 1000 (66.4%) correct signal events\n",
      "-- 876 of 1000 (87.6%) correct background events; 655 of 1000 (65.5%) correct signal events\n",
      "-- 883 of 1000 (88.3%) correct background events; 641 of 1000 (64.1%) correct signal events\n",
      "-- 889 of 1000 (88.9%) correct background events; 637 of 1000 (63.7%) correct signal events\n",
      "-- 895 of 1000 (89.5%) correct background events; 624 of 1000 (62.4%) correct signal events\n",
      "-- 898 of 1000 (89.8%) correct background events; 604 of 1000 (60.4%) correct signal events\n",
      "-- 905 of 1000 (90.5%) correct background events; 593 of 1000 (59.3%) correct signal events\n",
      "-- 909 of 1000 (90.9%) correct background events; 571 of 1000 (57.099999999999994%) correct signal events\n",
      "-- 916 of 1000 (91.60000000000001%) correct background events; 546 of 1000 (54.6%) correct signal events\n",
      "-- 923 of 1000 (92.30000000000001%) correct background events; 516 of 1000 (51.6%) correct signal events\n",
      "-- 929 of 1000 (92.9%) correct background events; 488 of 1000 (48.8%) correct signal events\n",
      "-- 934 of 1000 (93.4%) correct background events; 459 of 1000 (45.9%) correct signal events\n",
      "-- 939 of 1000 (93.89999999999999%) correct background events; 419 of 1000 (41.9%) correct signal events\n",
      "-- 946 of 1000 (94.6%) correct background events; 382 of 1000 (38.2%) correct signal events\n",
      "-- 958 of 1000 (95.8%) correct background events; 333 of 1000 (33.300000000000004%) correct signal events\n",
      "-- 964 of 1000 (96.39999999999999%) correct background events; 291 of 1000 (29.099999999999998%) correct signal events\n",
      "-- 970 of 1000 (97.0%) correct background events; 247 of 1000 (24.7%) correct signal events\n",
      "-- 977 of 1000 (97.7%) correct background events; 217 of 1000 (21.7%) correct signal events\n",
      "-- 979 of 1000 (97.89999999999999%) correct background events; 180 of 1000 (18.0%) correct signal events\n",
      "-- 984 of 1000 (98.4%) correct background events; 143 of 1000 (14.299999999999999%) correct signal events\n",
      "-- 991 of 1000 (99.1%) correct background events; 107 of 1000 (10.7%) correct signal events\n",
      "-- 995 of 1000 (99.5%) correct background events; 80 of 1000 (8.0%) correct signal events\n",
      "-- 996 of 1000 (99.6%) correct background events; 59 of 1000 (5.8999999999999995%) correct signal events\n",
      "-- 1000 of 1000 (100.0%) correct background events; 34 of 1000 (3.4000000000000004%) correct signal events\n",
      "-- 1000 of 1000 (100.0%) correct background events; 16 of 1000 (1.6%) correct signal events\n",
      "-- 1000 of 1000 (100.0%) correct background events; 9 of 1000 (0.8999999999999999%) correct signal events\n",
      "-- 1000 of 1000 (100.0%) correct background events; 6 of 1000 (0.6%) correct signal events\n",
      "-- 1000 of 1000 (100.0%) correct background events; 1 of 1000 (0.1%) correct signal events\n",
      "-- 1000 of 1000 (100.0%) correct background events; 0 of 1000 (0.0%) correct signal events\n"
     ]
    }
   ],
   "source": [
    "for thh in np.arange(0,1,0.01):\n",
    "    nts = 0; ntb = 0\n",
    "    ncs = 0; ncb = 0\n",
    "    for ye,yp in zip(y_e,y_pred):\n",
    "        if(ye == 0):\n",
    "            ntb += 1  # add one background event\n",
    "            if(yp < thh):\n",
    "                ncb += 1  # add one correctly predicted background event\n",
    "\n",
    "        if(ye == 1):\n",
    "            nts += 1  # add one signal event\n",
    "            if(yp >= thh):\n",
    "                ncs += 1  # add one correctly predicted signal event\n",
    "\n",
    "    print(\"-- {0} of {1} ({2}%) correct background events; {3} of {4} ({5}%) correct signal events\".format(ncb,ntb,1.0*ncb/ntb*100,ncs,nts,1.0*ncs/nts*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incep.save('models/largenet_noise.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- With of order 11k params, reached val loss of about 0.52 and seemed to slow significantly\n",
    "- With of order 30k params, reached val loss of about 0.49 at best\n",
    "- The problem of fluctuating validation loss was due to a learning rate that was too high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of 20x20 window table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate table consisting of 20x20 windows for each SiPM with maximum charge\n",
    "# -- Table is 2304x400: for (i,j) in (0,0) to (48,48); the table contains the list \n",
    "#     of IDs for the corresponding 20x20 window for each 1D SiPM ID = i*48 + j\n",
    "\n",
    "# Create the HDF5 file.\n",
    "h5f = h5py.File(\"wtbl.h5\")\n",
    "\n",
    "tbl = np.zeros([48*48,400])\n",
    "print(\"Generating window table...\")\n",
    "for i in range(48):\n",
    "    for j in range(48):\n",
    "        sipm_id = i*48 + j\n",
    "        \n",
    "        # Determine the 20x20 window.\n",
    "        i_in = i - 10; i_fi = i + 10\n",
    "        if(i_in < 0):\n",
    "            i_fi = (i - i_in) + 10\n",
    "            i_in = 0\n",
    "        elif(i_fi > 48):\n",
    "            i_in = i - (i_fi - 48) - 10\n",
    "            i_fi = 48\n",
    "        j_in = j - 10; j_fi = j + 10\n",
    "        if(j_in < 0):\n",
    "            j_fi = (j - j_in) + 10\n",
    "            j_in = 0\n",
    "        elif(j_fi > 48):\n",
    "            j_in = j - (j_fi - 48) - 10\n",
    "            j_fi = 48\n",
    "            \n",
    "        # Save the 20x20 window in the table.\n",
    "        nwin = 0\n",
    "        for iw in range(i_in,i_fi):\n",
    "            for jw in range(j_in,j_fi):\n",
    "                w_id = iw*48 + jw\n",
    "                tbl[sipm_id][nwin] = w_id\n",
    "                nwin += 1\n",
    "\n",
    "# Save the table to an HDF5 file.\n",
    "print(\"Saving table to file...\")\n",
    "h5f.create_dataset(\"wtbl\",data=tbl)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the energies of the training events.\n",
    "esums = []\n",
    "for earr in s_earray:\n",
    "    esums.append(np.sum(earr)/12.)\n",
    "#for earr in b_earray:\n",
    "#    esums.append(np.sum(earr))\n",
    "    \n",
    "fig = plt.figure();\n",
    "ax1 = fig.add_subplot(111);\n",
    "fig.set_figheight(5.0)\n",
    "fig.set_figwidth(7.5)\n",
    "\n",
    "plt.hist(esums, 100, normed=1, facecolor='green')\n",
    "plt.xlabel('Cathode charge')\n",
    "plt.ylabel('Counts/bin')\n",
    "plt.show()\n",
    "print(b_earray[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Carried over from NEW_kr_diff_mc_train.ipynb\n",
    "def NEW_SiPM_map_plot(xarr, yarr, plot_truth=True, normalize=True):\n",
    "    \"\"\"\n",
    "    Plots a SiPM map in the NEW Geometry\n",
    "    xarr is a NEW sipm map, yarr the pair of coordinates the map corresponds to\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        probs = (xarr - np.min(xarr))\n",
    "        probs /= np.max(probs)\n",
    "    else: \n",
    "        probs = xarr\n",
    "\n",
    "    fig = plt.figure();\n",
    "    ax1 = fig.add_subplot(111);\n",
    "    fig.set_figheight(7.0)\n",
    "    fig.set_figwidth(7.0)\n",
    "    ax1.axis([-250, 250, -250, 250]);\n",
    "\n",
    "    for i in range(48):\n",
    "        for j in range(48):\n",
    "            r = Ellipse(xy=(i * 10 - 235, j * 10 - 235), width=2., height=2.);\n",
    "            r.set_facecolor('0');\n",
    "            r.set_alpha(probs[i, j]);\n",
    "            ax1.add_artist(r);\n",
    "            \n",
    "    if plot_truth:\n",
    "        # Place a large blue circle for actual EL points.\n",
    "        xpt = yarr[0]\n",
    "        ypt = yarr[1]\n",
    "        mrk = Ellipse(xy=(xpt,ypt), width=4., height=4.);\n",
    "        mrk.set_facecolor('b');\n",
    "        ax1.add_artist(mrk);\n",
    "        #print(xpt,ypt)\n",
    "        \n",
    "    plt.xlabel(\"x (mm)\");\n",
    "    plt.ylabel(\"y (mm)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot training event slices.\n",
    "plt_nevt = 14900\n",
    "plt_nslice = 5\n",
    "\n",
    "plt_arr = x_t[plt_nevt,:,:,plt_nslice]\n",
    "NEW_SiPM_map_plot(plt_arr,[0, 0], False)\n",
    "chg_sum = np.sum(plt_arr)\n",
    "tot_chg_sum = np.sum(x_t[plt_nevt,:,:,:])\n",
    "max_chg = np.max(x_t[plt_nevt,:,:,plt_nslice])\n",
    "min_chg = np.min(x_t[plt_nevt,:,:,plt_nslice])\n",
    "print(\"Plotting event\", plt_nevt, \"slice\", plt_nslice, \"with charge sum\", chg_sum, \"and total sum\", tot_chg_sum,\n",
    "     \"max charge\", max_chg, \"and min charge\", min_chg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a histogram containing the SiPM distribution for a given (x,y,z), where x,y,z are indices in the map.\n",
    "def SiPM_dist(sipm_maps,x,y,z):\n",
    "    \n",
    "    return sipm_maps[:,x,y,z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine which SiPMs have all-zero distributions for a given slice.\n",
    "slnum = 0\n",
    "for sipmx in range(xdim):\n",
    "    for sipmy in range(ydim):\n",
    "        \n",
    "        dist = x_t[:,sipmx,sipmy,slnum]\n",
    "        nzeros = len(np.nonzero(dist)[0])\n",
    "        if(nzeros == 0):\n",
    "            print(\"All zeros for SiPM ({0},{1})\".format(sipmx,sipmy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot 12 distributions.\n",
    "fig = plt.figure();\n",
    "fig.set_figheight(20.0)\n",
    "fig.set_figwidth(13.0)\n",
    "\n",
    "for ndist in range(3):\n",
    "    sp = int(430 + ndist + 1)\n",
    "    print(\"plot\",sp)\n",
    "    ax = fig.add_subplot(sp);\n",
    "    \n",
    "    xv = np.random.randint(38) + 5\n",
    "    yv = np.random.randint(38) + 5 \n",
    "    sv = np.random.randint(14)\n",
    "\n",
    "    dist = x_t[:,xv,yv,sv]\n",
    "    plt.hist(dist[np.nonzero(dist)], 1000, normed=0, facecolor='green')\n",
    "    ax.set_yscale('log')\n",
    "    start, end = ax.get_xlim()\n",
    "    #ax.xaxis.set_ticks(np.arange(start, end, (end-start)/4.))\n",
    "    plt.xlim(0.0,0.05)\n",
    "    plt.xlabel('SiPM Charge')\n",
    "    plt.ylabel('Counts/bin')\n",
    "    plt.title('SiPM ({0},{1}); slice {2}'.format(xv,yv,sv))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot SiPM sum distributions for a range of events.\n",
    "#tr_evt_no = 15\n",
    "fig = plt.figure();\n",
    "fig.set_figheight(5.0)\n",
    "fig.set_figwidth(5.0)\n",
    "\n",
    "s_values = []\n",
    "for tr_evt_no in range(1000):\n",
    "    for nx in range(xdim):\n",
    "        for ny in range(ydim):\n",
    "            sval = np.sum(x_t[tr_evt_no,nx,ny,:])\n",
    "            if(sval != 0):\n",
    "                s_values.append(sval)\n",
    "        #print(\"SiPM at ({0},{1})\".format(nx,ny))\n",
    "\n",
    "ax = fig.add_subplot(111);\n",
    "plt.hist(s_values, 100, normed=0, facecolor='green')\n",
    "ax.set_yscale('log')\n",
    "#start, end = ax.get_xlim()\n",
    "#ax.xaxis.set_ticks(np.arange(start, end, (end-start)/4.))\n",
    "plt.xlim(0.0,0.3)\n",
    "plt.xlabel('SiPM Charge Sum')\n",
    "plt.ylabel('Counts/bin')\n",
    "#plt.title('Event {0}'.format(tr_evt_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(x_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old nets\n",
    "        cinputs = Convolution3D(32, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='relu',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution3D(32, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        \n",
    "        \n",
    "\n",
    "        inputs = Input(shape=(48, 48, 30, 1))\n",
    "        cinputs = Convolution3D(512, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='relu',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution3D(1024, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        #cinputs = Convolution3D(16, 3, 3, 3, border_mode='same', subsample=(2, 2, 2), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(32, 3, 3, 3, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(2, 2, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(512, 2, 2, 2, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='valid', subsample=(3, 3, 1), activation='relu')(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=1024, activation='relu', init='normal', W_regularizer=l2(0.001))(f1)\n",
    "        f1 = Dropout(.4)(f1)\n",
    "\n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='normal')(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        cinputs = Convolution3D(32, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='sigmoid',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        #cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='tanh',init='normal')(cinputs)\n",
    "        cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='sigmoid',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        #cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='tanh',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(16, 3, 3, 3, border_mode='same', subsample=(2, 2, 2), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(32, 3, 3, 3, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(2, 2, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(512, 2, 2, 2, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='valid', subsample=(3, 3, 1), activation='relu')(cinputs)\n",
    "\n",
    "        inputs = Input(shape=(48, 48, 30))\n",
    "        cinputs = Convolution2D(512, 6, 6, border_mode='same', subsample=(3, 3), activation='relu',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution2D(64, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution2D(1024, 2, 2, border_mode='same', subsample=(1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = Convolution2D(16, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        \n",
    "        # 2D net 16-12-16\n",
    "        inputs = Input(shape=(48, 48, 30))\n",
    "        cinputs = Convolution2D(32, 6, 6, border_mode='same', subsample=(3, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(inputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        #cinputs = Convolution2D(8, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.05))(cinputs)\n",
    "        cinputs = Convolution2D(64, 2, 2, border_mode='same', subsample=(1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = Convolution2D(128, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=128, activation='relu', init='lecun_uniform', W_regularizer=l2(0.008))(f1)\n",
    "        f1 = Dropout(.6)(f1)\n",
    "\n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='lecun_uniform', W_regularizer=l2(0.001))(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "        \n",
    "        # Good 3D net 16-12-16\n",
    "        inputs = Input(shape=(48, 48, 30, 1))\n",
    "        cinputs = Convolution3D(128, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = Convolution3D(128, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        #cinputs = Convolution3D(16, 3, 3, 3, border_mode='same', subsample=(2, 2, 2), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(32, 3, 3, 3, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(2, 2, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(512, 2, 2, 2, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='valid', subsample=(3, 3, 1), activation='relu')(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=64, activation='relu', init='lecun_uniform', W_regularizer=l2(0.02))(f1)\n",
    "        f1 = Dropout(.7)(f1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # DNN for 20x20x30\n",
    "        inputs = Input(shape=(xdim, ydim, zdim, 1))\n",
    "        cinputs = Convolution3D(256, 5, 5, 5, border_mode='same', subsample=(4, 4, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution3D(128, 2, 2, 3, border_mode='same', subsample=(2, 2, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(128, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=128, activation='relu', init='lecun_uniform', W_regularizer=l2(0.001))(f1)\n",
    "        f1 = Dropout(.7)(f1)\n",
    "\n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='normal', W_regularizer=l2(0.001))(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "\n",
    "        incep.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Nadam(lr=0.00001, beta_1=0.9, beta_2=0.999,\n",
    "                                      epsilon=1e-08, schedule_decay=0.1), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add noise\n",
    "nsigma = 1.0e-4\n",
    "\n",
    "inoise = x_t == 0.\n",
    "x_t[inoise] = np.abs(np.random.normal(0,1.0e-4,np.sum(inoise)))\n",
    "\n",
    "inoise = x_v == 0.\n",
    "x_v[inoise] = np.abs(np.random.normal(0,1.0e-4,np.sum(inoise)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
