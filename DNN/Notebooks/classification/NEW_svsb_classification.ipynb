{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal vs. background classification with NEW full MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.patches         import Ellipse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy  as np\n",
    "import random as rd\n",
    "import tables as tb\n",
    "import h5py\n",
    "import keras.backend.tensorflow_backend as K\n",
    "\n",
    "from __future__  import print_function\n",
    "from scipy.stats import threshold\n",
    "\n",
    "from keras.models               import Model, load_model\n",
    "from keras.layers               import Input, Dense, MaxPooling3D, AveragePooling3D, Convolution3D, Activation, Dropout, merge\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers           import SGD, Adam, Nadam         \n",
    "#from keras.callbacks            import ReduceLROnPlateau !!!!!\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core          import Flatten\n",
    "from keras                      import callbacks\n",
    "from keras.regularizers         import l2, activity_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading signal events...\n",
      "-- Reading file 25\n",
      "-- Reading file 26\n",
      "-- Reading file 27\n",
      "-- Reading file 28\n",
      "-- Reading file 29\n",
      "-- Reading file 30\n",
      "-- Reading file 31\n",
      "-- Reading file 32\n",
      "-- Reading file 33\n",
      "-- Reading file 34\n",
      "-- Reading file 35\n",
      "-- Reading file 36\n",
      "-- Reading file 37\n",
      "-- Reading file 38\n",
      "-- Reading file 39\n",
      "-- Reading file 40\n",
      "-- Reading file 41\n",
      "-- Reading file 42\n",
      "-- Reading file 43\n",
      "-- Reading file 44\n",
      "-- Reading file 45\n",
      "-- Reading file 46\n",
      "-- Reading file 47\n",
      "-- Reading file 48\n",
      "-- Reading file 49\n",
      "Reading background events...\n",
      "-- Reading file 25\n",
      "-- Reading file 26\n",
      "-- Reading file 27\n",
      "-- Reading file 28\n",
      "-- Reading file 29\n",
      "-- Reading file 30\n",
      "-- Reading file 31\n",
      "-- Reading file 32\n",
      "-- Reading file 33\n",
      "-- Reading file 34\n",
      "-- Reading file 35\n",
      "-- Reading file 36\n",
      "-- Reading file 37\n",
      "-- Reading file 38\n",
      "-- Reading file 39\n",
      "-- Reading file 40\n",
      "-- Reading file 41\n",
      "-- Reading file 42\n",
      "-- Reading file 43\n",
      "-- Reading file 44\n",
      "-- Reading file 45\n",
      "-- Reading file 46\n",
      "-- Reading file 47\n",
      "-- Reading file 48\n",
      "-- Reading file 49\n",
      "Read 47452 signal events and 44887 background events.\n"
     ]
    }
   ],
   "source": [
    "## TRAINING EVENTS\n",
    "f_start = 25\n",
    "f_end = 50\n",
    "# Read in the signal events.\n",
    "print(\"Reading signal events...\")\n",
    "for fn in range(f_start,f_end):\n",
    "    print(\"-- Reading file {0}\".format(fn))\n",
    "    s_dat = tb.open_file(\"/home/jrenner/data/classification/bb_1M_v0_08_07/hdf5_maps_NEW_training_MC_si_{0}.h5\".format(fn), 'r')\n",
    "    if(fn == f_start):\n",
    "        s_array = np.array(s_dat.root.maps)\n",
    "        s_energies = np.array(s_dat.root.energies)\n",
    "    else:\n",
    "        s_array = np.concatenate([s_array,np.array(s_dat.root.maps)])\n",
    "        s_energies = np.concatenate([s_energies,np.array(s_dat.root.energies)])\n",
    "        \n",
    "# Read in the background events.\n",
    "print(\"Reading background events...\")\n",
    "for fn in range(f_start,f_end):\n",
    "    print(\"-- Reading file {0}\".format(fn))\n",
    "    b_dat = tb.open_file(\"/home/jrenner/data/classification/se_1M_v0_08_07/hdf5_maps_NEW_training_MC_bg_{0}.h5\".format(fn), 'r')\n",
    "    if(fn == f_start):\n",
    "        b_array = np.array(b_dat.root.maps)\n",
    "        b_energies = np.array(b_dat.root.energies)\n",
    "    else:\n",
    "        b_array = np.concatenate([b_array,np.array(b_dat.root.maps)])\n",
    "        b_energies = np.concatenate([b_energies,np.array(b_dat.root.energies)])\n",
    "        \n",
    "print(\"Read {0} signal events and {1} background events.\".format(len(s_array),len(b_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dimensions and numbers of events\n",
    "Ntrain = 40000     # number of training events per sample\n",
    "Ntot = 44000\n",
    "xdim = 20\n",
    "ydim = 20\n",
    "zdim = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n"
     ]
    }
   ],
   "source": [
    "# Set up training (t) and validation (v) datasets.\n",
    "x_t = s_array[:Ntrain]\n",
    "x_v = s_array[Ntrain:Ntot]\n",
    "y_t = np.ones([Ntrain, 1])\n",
    "y_v = np.ones([Ntot-Ntrain, 1])\n",
    "\n",
    "print(\"Concatenating datasets...\")\n",
    "x_t = np.concatenate([x_t, b_array[:Ntrain]])\n",
    "x_v = np.concatenate([x_v, b_array[Ntrain:Ntot]])\n",
    "y_bt = np.zeros([Ntrain, 1])\n",
    "y_t = np.concatenate([y_t, y_bt])\n",
    "y_bv = np.zeros([Ntot-Ntrain, 1])\n",
    "y_v = np.concatenate([y_v, y_bv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaping...\n",
      "Prepared 80000 training events,  8000 validation events\n"
     ]
    }
   ],
   "source": [
    "# Include the final dimension (single-channel).\n",
    "print(\"Reshaping...\")\n",
    "#x_t = np.expand_dims(x_t, axis=1)\n",
    "#x_v = np.expand_dims(x_v, axis=1)\n",
    "x_t = np.reshape(x_t, (len(x_t), xdim, ydim, zdim, 1))\n",
    "x_v = np.reshape(x_v, (len(x_v), xdim, ydim, zdim, 1))\n",
    "print(\"Prepared\", len(x_t), \"training events, \", len(x_v), \"validation events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove noise (simple cut).\n",
    "nthr = 5.0e-5\n",
    "inoise = x_t < nthr\n",
    "x_t[inoise] = 0\n",
    "\n",
    "inoise = x_v < nthr\n",
    "x_v[inoise] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove noise (complex cut).\n",
    "nsum_thr = 0.01\n",
    "\n",
    "# Remove noise from training events.\n",
    "print(\"Removing noise from training events...\")\n",
    "for enum in range(len(x_t)):\n",
    "    for nx in range(xdim):\n",
    "        for ny in range(ydim):\n",
    "            sval = np.sum(x_t[enum,nx,ny,:])\n",
    "            if(sval < nsum_thr):\n",
    "                x_t[enum,nx,ny,:] = 0\n",
    "                \n",
    "# Remove noise from validation events.\n",
    "print(\"Removing noise from validation events...\")\n",
    "for enum in range(len(x_v)):\n",
    "    for nx in range(xdim):\n",
    "        for ny in range(ydim):\n",
    "            sval = np.sum(x_v[enum,nx,ny,:])\n",
    "            if(sval < nsum_thr):\n",
    "                x_v[enum,nx,ny,:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 20, 20, 60, 1) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_1 (Convolution3D)  (None, 5, 5, 15, 256) 32256       input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling3d_1 (MaxPooling3D)    (None, 3, 3, 8, 256)  0           convolution3d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_1 (BatchNorma (None, 3, 3, 8, 256)  512         maxpooling3d_1[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_2 (Convolution3D)  (None, 3, 3, 8, 64)   16448       batchnormalization_1[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_3 (Convolution3D)  (None, 2, 2, 3, 128)  98432       convolution3d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_2 (BatchNorma (None, 2, 2, 3, 128)  256         convolution3d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution3d_4 (Convolution3D)  (None, 2, 2, 3, 128)  16512       batchnormalization_2[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling3d_2 (MaxPooling3D)    (None, 1, 1, 2, 128)  0           convolution3d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 256)           0           maxpooling3d_2[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 128)           32896       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 128)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1)             129         dropout_1[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 197441\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lmodel = True\n",
    "model_type = 0\n",
    "\n",
    "with K.tf.device('/gpu:1'):\n",
    "    K.set_session(K.tf.Session(config=K.tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)))\n",
    "    \n",
    "    if(lmodel):\n",
    "        incep = load_model('models/conv3d_classifier_20.h5')\n",
    "    elif(model_type == 0):\n",
    "\n",
    "        inputs = Input(shape=(xdim, ydim, zdim, 1))\n",
    "        cinputs = Convolution3D(256, 5, 5, 5, border_mode='same', subsample=(4, 4, 4), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution3D(128, 2, 2, 3, border_mode='same', subsample=(2, 2, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(128, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=128, activation='relu', init='lecun_uniform', W_regularizer=l2(0.001))(f1)\n",
    "        f1 = Dropout(.7)(f1)\n",
    "\n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='normal', W_regularizer=l2(0.001))(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "\n",
    "        incep.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Nadam(lr=0.000005, beta_1=0.9, beta_2=0.999,\n",
    "                                      epsilon=1e-08, schedule_decay=0.1), metrics=['accuracy'])\n",
    "    elif(model_type == 1):\n",
    "        \n",
    "        # Input layer\n",
    "        inputs = Input(shape=(xdim, ydim, zdim, 1))\n",
    "        \n",
    "        # Initial convolution and normalization.\n",
    "        cinputs = Convolution3D(64, 4, 4, 4, border_mode='same', subsample=(2, 2, 2), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        \n",
    "        # 2 inception layers\n",
    "        i1 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i2_1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i2_2 = Convolution3D(128, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i2_1)\n",
    "        i3_1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i3_2 = Convolution3D(64, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i3_1)\n",
    "        i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        i4_2 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i4_1)\n",
    "        cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        i1 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i2_1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i2_2 = Convolution3D(128, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i2_1)\n",
    "        i3_1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(cinputs)\n",
    "        i3_2 = Convolution3D(64, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i3_1)\n",
    "        i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        i4_2 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.004))(i4_1)\n",
    "        cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        # Max pooling\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        \n",
    "        # 2 more inception layers\n",
    "        #i1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_1 = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_2 = Convolution3D(64, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i2_1)\n",
    "        #i3_1 = Convolution3D(8, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i3_2 = Convolution3D(32, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i3_1)\n",
    "        #i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #i4_2 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i4_1)\n",
    "        #cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        #i1 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_1 = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_2 = Convolution3D(64, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i2_1)\n",
    "        #i3_1 = Convolution3D(8, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i3_2 = Convolution3D(32, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i3_1)\n",
    "        #i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #i4_2 = Convolution3D(32, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i4_1)\n",
    "        #cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        # Max pooling\n",
    "        #cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "\n",
    "        # 2 more inception layers\n",
    "        #i1 = Convolution3D(48, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_1 = Convolution3D(24, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_2 = Convolution3D(96, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i2_1)\n",
    "        #i3_1 = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i3_2 = Convolution3D(64, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i3_1)\n",
    "        #i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #i4_2 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i4_1)\n",
    "        #cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        #i1 = Convolution3D(48, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_1 = Convolution3D(24, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i2_2 = Convolution3D(96, 3, 3, 3, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i2_1)\n",
    "        #i3_1 = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(cinputs)\n",
    "        #i3_2 = Convolution3D(64, 5, 5, 5, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i3_1)\n",
    "        #i4_1 = MaxPooling3D(pool_size=(3, 3, 3), strides=(1, 1, 1), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #i4_2 = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform')(i4_1)\n",
    "        #cinputs = merge([i1, i2_2, i3_2, i4_2], mode='concat', concat_axis=4)\n",
    "        \n",
    "        # Average pooling\n",
    "        #cinputs = AveragePooling3D(pool_size=(4, 4, 5), strides=(4, 4, 5), border_mode='valid', dim_ordering='default')(cinputs)\n",
    "        \n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=16, activation='relu', init='lecun_uniform', W_regularizer=l2(0.0001))(f1)\n",
    "        f1 = Dropout(.6)(f1)\n",
    "        \n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='normal', W_regularizer=l2(0.0005))(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "\n",
    "        incep.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Nadam(lr=0.0005, beta_1=0.9, beta_2=0.999,\n",
    "                                      epsilon=1e-08, schedule_decay=0.01), metrics=['accuracy'])\n",
    "        \n",
    "            #callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min'), \n",
    "    lcallbacks = [callbacks.ModelCheckpoint('models/conv3d_classifier_20.h5', monitor='val_loss', save_best_only=True, mode='min')]            \n",
    "    incep.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 8000 samples\n",
      "Epoch 1/60\n",
      "80000/80000 [==============================] - 28s - loss: 0.8158 - acc: 0.5697 - val_loss: 0.6932 - val_acc: 0.5006\n",
      "Epoch 2/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.7572 - acc: 0.6662 - val_loss: 0.6107 - val_acc: 0.6975\n",
      "Epoch 3/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.7155 - acc: 0.6938 - val_loss: 0.5885 - val_acc: 0.7012\n",
      "Epoch 4/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.6943 - acc: 0.7046 - val_loss: 0.5746 - val_acc: 0.7144\n",
      "Epoch 5/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.6780 - acc: 0.7145 - val_loss: 0.5658 - val_acc: 0.7225\n",
      "Epoch 6/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.6629 - acc: 0.7248 - val_loss: 0.5559 - val_acc: 0.7327\n",
      "Epoch 7/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.6479 - acc: 0.7331 - val_loss: 0.5476 - val_acc: 0.7414\n",
      "Epoch 8/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.6334 - acc: 0.7413 - val_loss: 0.5373 - val_acc: 0.7475\n",
      "Epoch 9/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.6204 - acc: 0.7494 - val_loss: 0.5283 - val_acc: 0.7555\n",
      "Epoch 10/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.6081 - acc: 0.7574 - val_loss: 0.5218 - val_acc: 0.7587\n",
      "Epoch 11/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.5957 - acc: 0.7624 - val_loss: 0.5132 - val_acc: 0.7620\n",
      "Epoch 12/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.5856 - acc: 0.7693 - val_loss: 0.5106 - val_acc: 0.7650\n",
      "Epoch 13/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.5761 - acc: 0.7729 - val_loss: 0.5052 - val_acc: 0.7676\n",
      "Epoch 14/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.5506 - acc: 0.7836 - val_loss: 0.4879 - val_acc: 0.7772\n",
      "Epoch 18/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.5460 - acc: 0.7857 - val_loss: 0.4876 - val_acc: 0.7794\n",
      "Epoch 19/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.5403 - acc: 0.7879 - val_loss: 0.4834 - val_acc: 0.7829\n",
      "Epoch 20/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.5358 - acc: 0.7897 - val_loss: 0.4821 - val_acc: 0.7830\n",
      "Epoch 21/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.5321 - acc: 0.7923 - val_loss: 0.4819 - val_acc: 0.7840\n",
      "Epoch 22/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.5292 - acc: 0.7937 - val_loss: 0.4787 - val_acc: 0.7849\n",
      "Epoch 23/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.5249 - acc: 0.7952 - val_loss: 0.4768 - val_acc: 0.7857\n",
      "Epoch 24/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.5224 - acc: 0.7975 - val_loss: 0.4794 - val_acc: 0.7855\n",
      "Epoch 25/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.5183 - acc: 0.7985 - val_loss: 0.4754 - val_acc: 0.7880\n",
      "Epoch 26/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.5162 - acc: 0.7986 - val_loss: 0.4746 - val_acc: 0.7899\n",
      "Epoch 27/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.5128 - acc: 0.8006 - val_loss: 0.4731 - val_acc: 0.7904\n",
      "Epoch 28/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.5092 - acc: 0.8027 - val_loss: 0.4712 - val_acc: 0.7894\n",
      "Epoch 29/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.5076 - acc: 0.8038 - val_loss: 0.4704 - val_acc: 0.7897\n",
      "Epoch 30/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.5054 - acc: 0.8042 - val_loss: 0.4680 - val_acc: 0.7902\n",
      "Epoch 31/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.5018 - acc: 0.8060 - val_loss: 0.4675 - val_acc: 0.7912\n",
      "Epoch 32/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4995 - acc: 0.8072 - val_loss: 0.4729 - val_acc: 0.7859\n",
      "Epoch 33/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.4968 - acc: 0.8076 - val_loss: 0.4655 - val_acc: 0.7939\n",
      "Epoch 34/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.4927 - acc: 0.8091 - val_loss: 0.4700 - val_acc: 0.7915\n",
      "Epoch 35/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4922 - acc: 0.8106 - val_loss: 0.4641 - val_acc: 0.7947\n",
      "Epoch 36/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.4905 - acc: 0.8115 - val_loss: 0.4638 - val_acc: 0.7940\n",
      "Epoch 37/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.4883 - acc: 0.8129 - val_loss: 0.4656 - val_acc: 0.7956\n",
      "Epoch 38/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4857 - acc: 0.8144 - val_loss: 0.4784 - val_acc: 0.7817\n",
      "Epoch 39/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4841 - acc: 0.8147 - val_loss: 0.4620 - val_acc: 0.7944\n",
      "Epoch 40/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4825 - acc: 0.8160 - val_loss: 0.4623 - val_acc: 0.7960\n",
      "Epoch 41/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4797 - acc: 0.8153 - val_loss: 0.4641 - val_acc: 0.7950\n",
      "Epoch 42/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.4770 - acc: 0.8182 - val_loss: 0.4594 - val_acc: 0.7971\n",
      "Epoch 43/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4766 - acc: 0.8193 - val_loss: 0.4710 - val_acc: 0.7875\n",
      "Epoch 44/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4737 - acc: 0.8189 - val_loss: 0.4789 - val_acc: 0.7831\n",
      "Epoch 45/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.4722 - acc: 0.8200 - val_loss: 0.4782 - val_acc: 0.7881\n",
      "Epoch 46/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.4716 - acc: 0.8203 - val_loss: 0.4656 - val_acc: 0.7939\n",
      "Epoch 47/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.4685 - acc: 0.8216 - val_loss: 0.4582 - val_acc: 0.7964\n",
      "Epoch 48/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.4661 - acc: 0.8228 - val_loss: 0.4587 - val_acc: 0.7967\n",
      "Epoch 49/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.4657 - acc: 0.8238 - val_loss: 0.4627 - val_acc: 0.7967\n",
      "Epoch 50/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.4630 - acc: 0.8251 - val_loss: 0.4762 - val_acc: 0.7865\n",
      "Epoch 51/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4622 - acc: 0.8255 - val_loss: 0.4805 - val_acc: 0.7837\n",
      "Epoch 52/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4606 - acc: 0.8279 - val_loss: 0.4562 - val_acc: 0.8000\n",
      "Epoch 53/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4579 - acc: 0.8279 - val_loss: 0.4689 - val_acc: 0.7946\n",
      "Epoch 54/60\n",
      "80000/80000 [==============================] - 23s - loss: 0.4570 - acc: 0.8290 - val_loss: 0.4588 - val_acc: 0.7997\n",
      "Epoch 55/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4558 - acc: 0.8294 - val_loss: 0.4577 - val_acc: 0.8005\n",
      "Epoch 56/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.4533 - acc: 0.8300 - val_loss: 0.4801 - val_acc: 0.7827\n",
      "Epoch 57/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4514 - acc: 0.8307 - val_loss: 0.4561 - val_acc: 0.7999\n",
      "Epoch 58/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.4510 - acc: 0.8311 - val_loss: 0.4664 - val_acc: 0.7942\n",
      "Epoch 59/60\n",
      "80000/80000 [==============================] - 25s - loss: 0.4486 - acc: 0.8328 - val_loss: 0.4573 - val_acc: 0.8007\n",
      "Epoch 60/60\n",
      "80000/80000 [==============================] - 24s - loss: 0.4476 - acc: 0.8319 - val_loss: 0.4838 - val_acc: 0.7810\n"
     ]
    }
   ],
   "source": [
    "hist = incep.fit(x_t, y_t, shuffle=True, nb_epoch=60, batch_size=100, verbose=1, validation_data=(x_v, y_v), callbacks=lcallbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test signal events...\n",
      "-- Reading file 0\n",
      "-- Reading file 1\n",
      "-- Reading file 2\n",
      "-- Reading file 3\n",
      "-- Reading file 4\n",
      "-- Reading file 5\n",
      "-- Reading file 6\n",
      "-- Reading file 7\n",
      "-- Reading file 8\n",
      "-- Reading file 9\n",
      "-- Reading file 10\n",
      "-- Reading file 11\n",
      "-- Reading file 12\n",
      "-- Reading file 13\n",
      "-- Reading file 14\n",
      "-- Reading file 15\n",
      "-- Reading file 16\n",
      "-- Reading file 17\n",
      "-- Reading file 18\n",
      "-- Reading file 19\n",
      "-- Reading file 20\n",
      "-- Reading file 21\n",
      "-- Reading file 22\n",
      "-- Reading file 23\n",
      "-- Reading file 24\n",
      "Reading background events...\n",
      "-- Reading file 0\n",
      "-- Reading file 1\n",
      "-- Reading file 2\n",
      "-- Reading file 3\n",
      "-- Reading file 4\n",
      "-- Reading file 5\n",
      "-- Reading file 6\n",
      "-- Reading file 7\n",
      "-- Reading file 8\n",
      "-- Reading file 9\n",
      "-- Reading file 10\n",
      "-- Reading file 11\n",
      "-- Reading file 12\n",
      "-- Reading file 13\n",
      "-- Reading file 14\n",
      "-- Reading file 15\n",
      "-- Reading file 16\n",
      "-- Reading file 17\n",
      "-- Reading file 18\n",
      "-- Reading file 19\n",
      "-- Reading file 20\n",
      "-- Reading file 21\n",
      "-- Reading file 22\n",
      "-- Reading file 23\n",
      "-- Reading file 24\n",
      "Read 47412 test signal events and 44657 test background events.\n",
      "Concatenating datasets...\n"
     ]
    }
   ],
   "source": [
    "## TEST EVENTS\n",
    "f_start = 0\n",
    "f_end = 25\n",
    "# Read in the signal events.\n",
    "print(\"Reading test signal events...\")\n",
    "for fn in range(f_start,f_end):\n",
    "    print(\"-- Reading file {0}\".format(fn))\n",
    "    s_dat = tb.open_file(\"/home/jrenner/data/classification/bb_1M_v0_08_07/hdf5_maps_NEW_training_MC_si_{0}.h5\".format(fn), 'r')\n",
    "    if(fn == f_start):\n",
    "        stest_array = np.array(s_dat.root.maps)\n",
    "        stest_energies = np.array(s_dat.root.energies)\n",
    "    else:\n",
    "        stest_array = np.concatenate([stest_array,np.array(s_dat.root.maps)])\n",
    "        stest_energies = np.concatenate([stest_energies,np.array(s_dat.root.energies)])\n",
    "        \n",
    "# Read in the background events.\n",
    "print(\"Reading background events...\")\n",
    "for fn in range(f_start,f_end):\n",
    "    print(\"-- Reading file {0}\".format(fn))\n",
    "    b_dat = tb.open_file(\"/home/jrenner/data/classification/se_1M_v0_08_07/hdf5_maps_NEW_training_MC_bg_{0}.h5\".format(fn), 'r')\n",
    "    if(fn == f_start):\n",
    "        btest_array = np.array(b_dat.root.maps)\n",
    "        btest_energies = np.array(b_dat.root.energies)\n",
    "    else:\n",
    "        btest_array = np.concatenate([btest_array,np.array(b_dat.root.maps)])\n",
    "        btest_energies = np.concatenate([btest_energies,np.array(b_dat.root.energies)])\n",
    "\n",
    "print(\"Read {0} test signal events and {1} test background events.\".format(len(stest_array),len(btest_array)))\n",
    "\n",
    "# Concatenate the datasets\n",
    "print(\"Concatenating datasets...\")\n",
    "x_e = np.concatenate([stest_array, btest_array])\n",
    "y_e = np.concatenate([np.ones([len(stest_array),1]), np.zeros([len(btest_array),1])])\n",
    "x_e = np.reshape(x_e, (len(x_e), xdim, ydim, zdim, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92064/92069 [============================>.] - ETA: 0s[0.44966127113524063, 0.80558059716082508]\n"
     ]
    }
   ],
   "source": [
    "loss_and_metrics = incep.evaluate(x_e, y_e);\n",
    "y_pred = incep.predict(x_e, batch_size=100, verbose=0)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 43557 of 44657 (97.53678034798577%) correct background events; 11251 of 47412 (23.73027925419725%) correct signal events\n"
     ]
    }
   ],
   "source": [
    "for thh in [0.897818]:#np.arange(0,1,0.01):\n",
    "    nts = 0; ntb = 0\n",
    "    ncs = 0; ncb = 0\n",
    "    for ye,yp in zip(y_e,y_pred):\n",
    "        if(ye == 0):\n",
    "            ntb += 1  # add one background event\n",
    "            if(yp < thh):\n",
    "                ncb += 1  # add one correctly predicted background event\n",
    "\n",
    "        if(ye == 1):\n",
    "            nts += 1  # add one signal event\n",
    "            if(yp >= thh):\n",
    "                ncs += 1  # add one correctly predicted signal event\n",
    "\n",
    "    print(\"-- {0} of {1} ({2}%) correct background events; {3} of {4} ({5}%) correct signal events\".format(ncb,ntb,1.0*ncb/ntb*100,ncs,nts,1.0*ncs/nts*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "incep.save('models/largenet_noise.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- With of order 11k params, reached val loss of about 0.52 and seemed to slow significantly\n",
    "- With of order 30k params, reached val loss of about 0.49 at best\n",
    "- The problem of fluctuating validation loss was due to a learning rate that was too high"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of 20x20 window table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate table consisting of 20x20 windows for each SiPM with maximum charge\n",
    "# -- Table is 2304x400: for (i,j) in (0,0) to (48,48); the table contains the list \n",
    "#     of IDs for the corresponding 20x20 window for each 1D SiPM ID = i*48 + j\n",
    "\n",
    "# Create the HDF5 file.\n",
    "h5f = h5py.File(\"wtbl.h5\")\n",
    "\n",
    "tbl = np.zeros([48*48,400])\n",
    "print(\"Generating window table...\")\n",
    "for i in range(48):\n",
    "    for j in range(48):\n",
    "        sipm_id = i*48 + j\n",
    "        \n",
    "        # Determine the 20x20 window.\n",
    "        i_in = i - 10; i_fi = i + 10\n",
    "        if(i_in < 0):\n",
    "            i_fi = (i - i_in) + 10\n",
    "            i_in = 0\n",
    "        elif(i_fi > 48):\n",
    "            i_in = i - (i_fi - 48) - 10\n",
    "            i_fi = 48\n",
    "        j_in = j - 10; j_fi = j + 10\n",
    "        if(j_in < 0):\n",
    "            j_fi = (j - j_in) + 10\n",
    "            j_in = 0\n",
    "        elif(j_fi > 48):\n",
    "            j_in = j - (j_fi - 48) - 10\n",
    "            j_fi = 48\n",
    "            \n",
    "        # Save the 20x20 window in the table.\n",
    "        nwin = 0\n",
    "        for iw in range(i_in,i_fi):\n",
    "            for jw in range(j_in,j_fi):\n",
    "                w_id = iw*48 + jw\n",
    "                tbl[sipm_id][nwin] = w_id\n",
    "                nwin += 1\n",
    "\n",
    "# Save the table to an HDF5 file.\n",
    "print(\"Saving table to file...\")\n",
    "h5f.create_dataset(\"wtbl\",data=tbl)\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Miscellaneous plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the energies of the training events.\n",
    "esums = []\n",
    "for earr in s_earray:\n",
    "    esums.append(np.sum(earr)/12.)\n",
    "#for earr in b_earray:\n",
    "#    esums.append(np.sum(earr))\n",
    "    \n",
    "fig = plt.figure();\n",
    "ax1 = fig.add_subplot(111);\n",
    "fig.set_figheight(5.0)\n",
    "fig.set_figwidth(7.5)\n",
    "\n",
    "plt.hist(esums, 100, normed=1, facecolor='green')\n",
    "plt.xlabel('Cathode charge')\n",
    "plt.ylabel('Counts/bin')\n",
    "plt.show()\n",
    "print(b_earray[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Carried over from NEW_kr_diff_mc_train.ipynb\n",
    "def NEW_SiPM_map_plot(xarr, yarr, plot_truth=True, normalize=True):\n",
    "    \"\"\"\n",
    "    Plots a SiPM map in the NEW Geometry\n",
    "    xarr is a NEW sipm map, yarr the pair of coordinates the map corresponds to\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        probs = (xarr - np.min(xarr))\n",
    "        probs /= np.max(probs)\n",
    "    else: \n",
    "        probs = xarr\n",
    "\n",
    "    fig = plt.figure();\n",
    "    ax1 = fig.add_subplot(111);\n",
    "    fig.set_figheight(7.0)\n",
    "    fig.set_figwidth(7.0)\n",
    "    ax1.axis([-250, 250, -250, 250]);\n",
    "\n",
    "    for i in range(48):\n",
    "        for j in range(48):\n",
    "            r = Ellipse(xy=(i * 10 - 235, j * 10 - 235), width=2., height=2.);\n",
    "            r.set_facecolor('0');\n",
    "            r.set_alpha(probs[i, j]);\n",
    "            ax1.add_artist(r);\n",
    "            \n",
    "    if plot_truth:\n",
    "        # Place a large blue circle for actual EL points.\n",
    "        xpt = yarr[0]\n",
    "        ypt = yarr[1]\n",
    "        mrk = Ellipse(xy=(xpt,ypt), width=4., height=4.);\n",
    "        mrk.set_facecolor('b');\n",
    "        ax1.add_artist(mrk);\n",
    "        #print(xpt,ypt)\n",
    "        \n",
    "    plt.xlabel(\"x (mm)\");\n",
    "    plt.ylabel(\"y (mm)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot training event slices.\n",
    "plt_nevt = 14900\n",
    "plt_nslice = 5\n",
    "\n",
    "plt_arr = x_t[plt_nevt,:,:,plt_nslice]\n",
    "NEW_SiPM_map_plot(plt_arr,[0, 0], False)\n",
    "chg_sum = np.sum(plt_arr)\n",
    "tot_chg_sum = np.sum(x_t[plt_nevt,:,:,:])\n",
    "max_chg = np.max(x_t[plt_nevt,:,:,plt_nslice])\n",
    "min_chg = np.min(x_t[plt_nevt,:,:,plt_nslice])\n",
    "print(\"Plotting event\", plt_nevt, \"slice\", plt_nslice, \"with charge sum\", chg_sum, \"and total sum\", tot_chg_sum,\n",
    "     \"max charge\", max_chg, \"and min charge\", min_chg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Return a histogram containing the SiPM distribution for a given (x,y,z), where x,y,z are indices in the map.\n",
    "def SiPM_dist(sipm_maps,x,y,z):\n",
    "    \n",
    "    return sipm_maps[:,x,y,z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Determine which SiPMs have all-zero distributions for a given slice.\n",
    "slnum = 0\n",
    "for sipmx in range(xdim):\n",
    "    for sipmy in range(ydim):\n",
    "        \n",
    "        dist = x_t[:,sipmx,sipmy,slnum]\n",
    "        nzeros = len(np.nonzero(dist)[0])\n",
    "        if(nzeros == 0):\n",
    "            print(\"All zeros for SiPM ({0},{1})\".format(sipmx,sipmy))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot 12 distributions.\n",
    "fig = plt.figure();\n",
    "fig.set_figheight(20.0)\n",
    "fig.set_figwidth(13.0)\n",
    "\n",
    "for ndist in range(3):\n",
    "    sp = int(430 + ndist + 1)\n",
    "    print(\"plot\",sp)\n",
    "    ax = fig.add_subplot(sp);\n",
    "    \n",
    "    xv = np.random.randint(38) + 5\n",
    "    yv = np.random.randint(38) + 5 \n",
    "    sv = np.random.randint(14)\n",
    "\n",
    "    dist = x_t[:,xv,yv,sv]\n",
    "    plt.hist(dist[np.nonzero(dist)], 1000, normed=0, facecolor='green')\n",
    "    ax.set_yscale('log')\n",
    "    start, end = ax.get_xlim()\n",
    "    #ax.xaxis.set_ticks(np.arange(start, end, (end-start)/4.))\n",
    "    plt.xlim(0.0,0.05)\n",
    "    plt.xlabel('SiPM Charge')\n",
    "    plt.ylabel('Counts/bin')\n",
    "    plt.title('SiPM ({0},{1}); slice {2}'.format(xv,yv,sv))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot SiPM sum distributions for a range of events.\n",
    "#tr_evt_no = 15\n",
    "fig = plt.figure();\n",
    "fig.set_figheight(5.0)\n",
    "fig.set_figwidth(5.0)\n",
    "\n",
    "s_values = []\n",
    "for tr_evt_no in range(1000):\n",
    "    for nx in range(xdim):\n",
    "        for ny in range(ydim):\n",
    "            sval = np.sum(x_t[tr_evt_no,nx,ny,:])\n",
    "            if(sval != 0):\n",
    "                s_values.append(sval)\n",
    "        #print(\"SiPM at ({0},{1})\".format(nx,ny))\n",
    "\n",
    "ax = fig.add_subplot(111);\n",
    "plt.hist(s_values, 100, normed=0, facecolor='green')\n",
    "ax.set_yscale('log')\n",
    "#start, end = ax.get_xlim()\n",
    "#ax.xaxis.set_ticks(np.arange(start, end, (end-start)/4.))\n",
    "plt.xlim(0.0,0.3)\n",
    "plt.xlabel('SiPM Charge Sum')\n",
    "plt.ylabel('Counts/bin')\n",
    "#plt.title('Event {0}'.format(tr_evt_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(x_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old nets\n",
    "        cinputs = Convolution3D(32, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='relu',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution3D(32, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        \n",
    "        \n",
    "\n",
    "        inputs = Input(shape=(48, 48, 30, 1))\n",
    "        cinputs = Convolution3D(512, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='relu',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution3D(1024, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        #cinputs = Convolution3D(16, 3, 3, 3, border_mode='same', subsample=(2, 2, 2), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(32, 3, 3, 3, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(2, 2, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(512, 2, 2, 2, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='valid', subsample=(3, 3, 1), activation='relu')(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=1024, activation='relu', init='normal', W_regularizer=l2(0.001))(f1)\n",
    "        f1 = Dropout(.4)(f1)\n",
    "\n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='normal')(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        cinputs = Convolution3D(32, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='sigmoid',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        #cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='tanh',init='normal')(cinputs)\n",
    "        cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='sigmoid',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        #cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='tanh',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(16, 3, 3, 3, border_mode='same', subsample=(2, 2, 2), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(32, 3, 3, 3, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(2, 2, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(512, 2, 2, 2, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='valid', subsample=(3, 3, 1), activation='relu')(cinputs)\n",
    "\n",
    "        inputs = Input(shape=(48, 48, 30))\n",
    "        cinputs = Convolution2D(512, 6, 6, border_mode='same', subsample=(3, 3), activation='relu',init='normal', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution2D(64, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution2D(1024, 2, 2, border_mode='same', subsample=(1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = Convolution2D(16, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='normal', W_regularizer=l2(0.001))(cinputs)\n",
    "        \n",
    "        # 2D net 16-12-16\n",
    "        inputs = Input(shape=(48, 48, 30))\n",
    "        cinputs = Convolution2D(32, 6, 6, border_mode='same', subsample=(3, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(inputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(3, 3), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        #cinputs = Convolution2D(8, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.05))(cinputs)\n",
    "        cinputs = Convolution2D(64, 2, 2, border_mode='same', subsample=(1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = Convolution2D(128, 1, 1, border_mode='same', subsample=(1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=3, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=128, activation='relu', init='lecun_uniform', W_regularizer=l2(0.008))(f1)\n",
    "        f1 = Dropout(.6)(f1)\n",
    "\n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='lecun_uniform', W_regularizer=l2(0.001))(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "        \n",
    "        # Good 3D net 16-12-16\n",
    "        inputs = Input(shape=(48, 48, 30, 1))\n",
    "        cinputs = Convolution3D(128, 6, 6, 6, border_mode='same', subsample=(3, 3, 5), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 3), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = Convolution3D(128, 2, 2, 2, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        #cinputs = Convolution3D(16, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.01))(cinputs)\n",
    "        #cinputs = Convolution3D(16, 3, 3, 3, border_mode='same', subsample=(2, 2, 2), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(32, 3, 3, 3, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(64, 2, 2, 2, border_mode='same', subsample=(2, 2, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(512, 2, 2, 2, border_mode='same', subsample=(2, 2, 5), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='normal')(cinputs)\n",
    "        #cinputs = Convolution3D(1024, 2, 2, 1, border_mode='valid', subsample=(3, 3, 1), activation='relu')(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=64, activation='relu', init='lecun_uniform', W_regularizer=l2(0.02))(f1)\n",
    "        f1 = Dropout(.7)(f1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # DNN for 20x20x30\n",
    "        inputs = Input(shape=(xdim, ydim, zdim, 1))\n",
    "        cinputs = Convolution3D(256, 5, 5, 5, border_mode='same', subsample=(4, 4, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(inputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(3, 3, 3), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(64, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = Convolution3D(128, 2, 2, 3, border_mode='same', subsample=(2, 2, 3), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = BatchNormalization(epsilon=1e-05, mode=0, axis=4, momentum=0.99, weights=None, beta_init='zero', gamma_init='one', gamma_regularizer=None, beta_regularizer=None)(cinputs)\n",
    "        cinputs = Convolution3D(128, 1, 1, 1, border_mode='same', subsample=(1, 1, 1), activation='relu',init='lecun_uniform', W_regularizer=l2(0.001))(cinputs)\n",
    "        cinputs = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='same', dim_ordering='default')(cinputs)\n",
    "        f1 = Flatten()(cinputs)\n",
    "        f1 = Dense(output_dim=128, activation='relu', init='lecun_uniform', W_regularizer=l2(0.001))(f1)\n",
    "        f1 = Dropout(.7)(f1)\n",
    "\n",
    "        inc_output = Dense(output_dim=1, activation='sigmoid',init='normal', W_regularizer=l2(0.001))(f1)\n",
    "        incep = Model(inputs, inc_output)\n",
    "\n",
    "        incep.compile(loss='binary_crossentropy',\n",
    "                      optimizer=Nadam(lr=0.00001, beta_1=0.9, beta_2=0.999,\n",
    "                                      epsilon=1e-08, schedule_decay=0.1), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add noise\n",
    "nsigma = 1.0e-4\n",
    "\n",
    "inoise = x_t == 0.\n",
    "x_t[inoise] = np.abs(np.random.normal(0,1.0e-4,np.sum(inoise)))\n",
    "\n",
    "inoise = x_v == 0.\n",
    "x_v[inoise] = np.abs(np.random.normal(0,1.0e-4,np.sum(inoise)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# OLD READ METHOD\n",
    "\n",
    "# Signal events.\n",
    "s_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_si_20.h5', 'r')\n",
    "#s_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_si_nst_nonorm.h5', 'r')\n",
    "print(s_dat)\n",
    "s_array = np.array(s_dat.root.maps)\n",
    "x_t = s_array[:Ntrain]\n",
    "x_v = s_array[Ntrain:Ntot-Ntest]\n",
    "x_e = s_array[Ntot-Ntest:Ntot]\n",
    "y_t = np.ones([Ntrain, 1])\n",
    "y_v = np.ones([Ntot-Ntrain-Ntest, 1])\n",
    "y_e = np.ones([Ntest, 1])\n",
    "\n",
    "s_earray = np.array(s_dat.root.energies)\n",
    "\n",
    "# Background events.\n",
    "b_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_bg_20.h5', 'r')\n",
    "#b_dat = tb.open_file('/home/jrenner/data/classification/NEW_training_MC_bg_nst_nonorm.h5', 'r')\n",
    "print(b_dat)\n",
    "b_array = np.array(b_dat.root.maps)\n",
    "print(\"Concatenating datasets...\")\n",
    "x_t = np.concatenate([x_t, b_array[:Ntrain]])\n",
    "x_v = np.concatenate([x_v, b_array[Ntrain:Ntot-Ntest]])\n",
    "x_e = np.concatenate([x_e, b_array[Ntot-Ntest:Ntot]])\n",
    "y_bt = np.zeros([Ntrain, 1])\n",
    "y_t = np.concatenate([y_t, y_bt])\n",
    "y_bv = np.zeros([Ntot-Ntrain-Ntest, 1])\n",
    "y_v = np.concatenate([y_v, y_bv])\n",
    "y_be = np.zeros([Ntest, 1])\n",
    "y_e = np.concatenate([y_e, y_be])\n",
    "\n",
    "b_earray = np.array(b_dat.root.energies)\n",
    "\n",
    "# Normalize\n",
    "#mval = max(np.max(s_array),np.max(b_array))\n",
    "#muval = np.mean(s_array)\n",
    "#sval = np.std(s_array)\n",
    "#print(\"Normalizing with max value of\", mval, \"(mean of\", muval, \"; sigma of \", sval, \")\")\n",
    "#x_t /= sval\n",
    "#x_v /= sval"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
